{"audio_filepath": "data/final_audios/0DKOUFrP7xI.wav", "duration": 167.42, "text": "music so now so this is what the progression was rate that in two thousand and six people started or the study by hinton and others led to the survival and then people started realizing that deep neural networks and actually we use for a lot of practical applications and actually beat a lot of existing systems but theres still some problems and we still need to make the system more robust faster and even scale higher accuracies and so on right so impera nearly vile there was a lot of success happening from two thousand and twelve to two thousand and sixteen or even two thousand and ten to two thousand and sixteen in peril there was also a lot of research to find better optimization algorithms which could lead to better convergence better accuracies and again some of the older ideas which were proposed way back in one thousand, nine hundred and eighty-three now this is again something that we will do in the course so most of the things that i am talking about we are going to cover in the course so we are going to talk about the amazing it challenge theyre going to talk about all those networks the winning networks that i had listed there alex needs a definite google net and so on we are going to talk about nesterov gradient descent which is listed on this slide and many other better optimization methods which were proposed starting from two thousand and eleven so there was this parallel resource happening while people were getting a lot of success using traditional neural networks theres also interested in making them better and robust and lead for lead to faster convergence and better accuracies and so on so this led to a lot of interest in coming up with better optimization algorithms and there was a series of these proposed starting from two thousand and eleven so a degrade is again something that well do in the course rmsprop adam eve and many more right so many new algorithms have been proposed and in parallel a lot of other regularization techniques or weight initialization strategies have also been proposed for example batch normalization or xaviar initialization and so on so these are all things which were aimed at making neural networks perform even better or faster and even reach better solutions or better accuracies and so on so this is all that we are going to see in the course at some point of the other music you"}
{"audio_filepath": "data/final_audios/4TC5s_xNKSs.wav", "duration": 417.62, "text": "music hello everyone welcome to lecture one of cs seven zero one five which is the course on deep learning todays lecture is going to be a bit non technical were not going to cover any technical concepts we only going to talk about a brief or partial history of deep learning so we hear the terms artificial neural networks artificial neurons quite often these days and i just wanted you take you through the journey of where does all this originate from and this history contains several spans across several fields not just computer science we will start with biology then talk about something in physics then eventually come to computer science and so on right so with that lets start so just some acknowledgments and disclaimers so i have taken a lot of this material from the first paper which i have mentioned on the bullet and there might still be some errors because its dates as black as one thousand, eight hundred and seventy-one so maybe i have got some of the facts wrong so feel free to contact me if you think some of these portions need to be corrected and it would be good if you could provide me appropriate references for these corrections so lets start with the first chapter which is on biological neurons as i said it spans several fields so well start with biology and i will first talk about the brain and neurons within the brain right so way back in one thousand, eight hundred and seventy-one one thousand, eight hundred and seventy-three around that time joseph von gaillarde actually proposed that the nervous system our nervous system is a single continuous network as opposed to a network of many discrete cells right so his idea was that this is one gigantic cell sitting in our nervous system and its not a network of discrete cells and this theory was known as the reticular theory right and around the same time there was this some breakthrough or some dis some progress in staining techniques where camillo golgi discovered that a chemical reaction that will allow you to examine the neurons new neurons or the nervous tissue right so he was looking at this nervous tissue using some staining technique and by looking at what you see in this figure on the right hand side the yellow figure even he concluded that this is just once single cell and not a network of discrete cells right so he was again a proponent of reticular theory so this is about camilla corgi and then interestingly santiago kayal he used the same technique with gaul he proposed and he studied the same tissue and he came up with the conclusion that this is not a single cell this is actually a collection of various discrete cells which to get the forms and network so it is a network of things as opposed to a single cell there right so thats what his theory was and this was eventually came to be known as the neuron doctrine although this was not consolidated in the form of a doctrine by kayal that was done by this gentleman so he coined the term neuron so now today when you think about art here about artificial neural networks or artificial neurons the term neuron actually originated way back in one thousand, eight hundred and ninety-one and this gentleman was responsible for coining that and he was also responsible for consolidating the neuron doctrine so already as you saw on the previous slide chirality proposed it but then over the years many people bought this idea and this guy was responsible for consolidating that into a neuron doctrine interestingly he is not only responsible for coining the term neuron he is also responsible for coining the term chromosome so two very important terms were coined by this one person right so now heres a question right so around one thousand, nine hundred and six when it was time to give the nobel prize in medicine what do you think which of these two proponents say there are two theories one is reticular theory which is a single cell and then theres this neuron doctrine which is a collection of cells our collection of neurons that a nervous system is a collection of neurons so what do you think which of these two guys who are proponents of these two different theories who would have got the actual nobel prize for medicine so interestingly it was given to both of them so till one thousand, nine hundred and six in fact way later till one thousand, nine hundred and fifty also this debate was not completely sad settled and then the committee said okay both of these are interesting pieces of work we yet dont know what really actually what the situation is actually but these conflicting ideas have a place together and so the nobel prize was actually given to both of them and this led to a history of some kind of controversies between these two times scientists and so on and this debate surprisingly was settled way later in one thousand, nine hundred and fifty and not by progress in biology as such but by progress in a different field so this was with the advent of electron microscopy so now it was able to see this at a much better scale and by looking at this under a microscope it was found that actually there is a gap between these neurons and hence its not a one single cell its actually a collection or a network of cells with a clear gap between them or some connections between them which are now known as synapses right so this was when the debate was settled so now why am i talking about biology why am i telling you about biological neuron and so on right so this is what we need to understand so there has always been interested in understanding how the human brain works from a biological perspective at least and around this time the debate was more or less settled that we have this our brain is a collection of many neurons and they interact with each other to help us do a lot of complex processing that we do on a daily basis right right from getting up in the morning and deciding what do we want to do today taking decisions performing computations and various complex things that our brain is capable of doing right now their interest is in seeing if you understand how the brain works can we make an artificial model for that right so can we come up with something which can simulate how our brain works and whats that model and how do we make a computer do that or how do we make a machine do that so thats why i started from biological neurons to take the inspiration from biology music music"}
{"audio_filepath": "data/final_audios/6USgwLa-7ks.wav", "duration": 211.7, "text": "music ill talk about the history of convolutional neural networks and i call this part of history as cats and itll become obvious why i call it soso around one thousand, nine hundred and fifty-nine who billion weasel did this famous experiment there still i think you could see some videos of it on youtube where theres this cat and it was a screen in front of it and ill screen there are these lines being displayed at different locations and in different orientation so its a slanted horizontal vertical and so on and there are some electrodes fitted to the cat and they were measuring trying to measure that which parts of brain actually respond to different visual stimuli its if you show it a stimulus at a certain location theres a different part of the brain fire and so on its oh and one of the things of outcomes of the study was that that different neurons in brain fire only different types of stimuli its not that all neurons in brain always fire any kind of visual stimuli that you give to them right so this is essentially roughly the idea behind convolutional neural networks starting from something known as neurocognitive which was proposed way back in one thousand, nine hundred and eighty you could think of it as a very primitive convolutional neural network im sure that most of you have now read about or heard about convolutional neural networks but something very similar to it was proposed way back in one thousand, nine hundred and eighty and what we know as the modern car emulation the neural networks maybe i think yan li kun is someone who proposed them way back in one thousand, nine hundred and eighty-nine and he was interested in using them for the task of handwritten digit recognition and this was again in the context of postal delivery services it so a lot of pin codes get written or phone numbers get written on the postcards and theres a requirement to read them automatically so that they can be the letters or postcards can be separated into different categories according to the postcard and according the postal code and so on its so or the pin code so thats where this interest was there and one thousand, nine hundred and eighty-nine was when this convolutional neural networks were first proposed or used for the stars and then over the years several improvements were done to that and in one thousand, nine hundred and ninety-eight this now how famous data set the emulous data set which is used for teaching deep neural networks courses or even for initial experiments with various neural network based networks this is one of the popular data sets which is used in this field and this was again released way back in one thousand, nine hundred and ninety-eight and even today even for mykos i use it for various assignments and so on so its interesting that an algorithm which was inspired by an experiment on cats is today used to detect cats and we do of course among other various other things is just im just jokingly saying this music you"}
{"audio_filepath": "data/final_audios/WpR8eOLUo9Q.wav", "duration": 823.18, "text": "music and now what well get into uh in the next chapter is well start talking about artificial intelligence and this is titled as from the spring to the winter of ai so im going to talk about when was this boom in ai started or when is that people started thinking and talking about ai seriously and what event happened to the initial boom and so on right so lets start with one thousand, nine hundred and forty-three where as i was saying that there was a lot of interest in understanding how does the human brain work and then come up with a computational or a mathematical model of that right so molik and pits one of them was a neuroscientist and the other one was a logician right so no computer scientists or anything at that point of time and they came up with this extremely simplified model that just as a brain takes the input from lot of fact s right so now suppose you want to decide whether you want to go out for a movie or not so you would probably think about do you really have any exams coming up that could be your factor xone you could think about is the weather good to go out is it training it would be difficult to go out at this point would there be a lot of traffic is it a very popular movie and hence tickets may not be available and so on right so a brain kind of processes all this information you might also look at things like the reviews of the movie or the imdb rating of the movie and so on and based on all these complex inputs it applies some function and then takes a decision yes or no that okay i want to probably go for a movie so this is an overly simplified model of how the brain works is and what this model says is that you take inputs from various sources and based on that you come up with a binary decision right so this is what they proposed in one thousand, nine hundred and forty-three so now we have come to an artificial neurons so this is not a biological neuron this is how you would implement it as a machine right so that was in one hundred and ninety-four three then along and then this kind of led to a lot of uh boom uh in our interest in artificial intelligence and so on and i guess around one thousand, nine hundred and fifty-six uh in a conference the term artificial intelligence was forly coined and within a one or two years from there frank rosenal came up with this perceptron model of uh doing computations or perceptron model of what an artificial neuron could be and well talk about this in detail later on the course i tell you what these things are as of now just think of the a new model was proposed and this is what he had to say about this model right so he said that the perceptron may eventually be able to learn make decisions and translate languages you find anything odd about this statement yeah so learn and make decisions make sense but why translate languages why so specific why such a specific interest in languages right so that you have to connect back to history right so this is also the the period of the cold war and theres always a lot of interest a lot of research in translation was actually fueled by the world war and events that happened after that where these uh countries which were at loggerheads with each other they wanted to understand what the other countries doing but they did not speak each others language thats why there was a lot of interest from spon point of view or from spying and so on to be able to translate languages and hence that specific required and a lot of this research would have been funded from agencies which are interested in these things right in the defense or war or something okay so and this work was largely done for the navy and this is an uh this is an extract from the article written in new york times way back in one thousand, nine hundred and fifty-seven or fifty-eight where it was mentioned that the embryo of an this perceptron is an embryo of an electronic computer that the navy expects will be able to walk talk see write reproduce itself and be conscious of it existence so im not quoting something from two thousand and seventeen or eighteen this is way back in one thousand, nine hundred and fifty-seven fifty-eight right and thats why i like the history part of it so recently theres a lot of uh boom or a lot of hype around ai that ai will take over a lot of things itll take over jobs it might eventually uh we might be colonized by ai agents and so on so i just want to emphasize that i dont know whether that will happen or not but this is not something new we have been talking about the promise of ai as far back since one thousand, nine hundred and fifty-seven one thousand, nine hundred and fifty-eight right this is not something new that people are talking about now its always been there and to what extent this promise will be fulfilled is yet to be seen and of course as compared to one thousand, nine hundred and fifty-seven fifty-eight we have made a lot of progress in other fields which have enabled ai to be uh much more successful than it was earlier for example we have much better compute power now we have lots of data now thanks to the internet and other things that you can actually crawl tons and tons of data and then try to learn something from a data or try to make the machine learn something from data right so we have made a lot of progress in other aspects because of which ai is now at a position where it can really make a difference but just wanted to say that these are not things which have not been said in the past it has always been that a has always been considered to be very promising and perhaps a bit hyped also right so thats about one thousand, nine hundred and fifty-seven fifty-eight then now what we talk about or the for the past eight to ten years at least when we talk about ai talking about deep learning and that is what this course is about largely about deep learning im not saying that other and what deep learning uh is largely about if i want to tell you in a very layman nutshell term is its about a large number of artificial neurons connected to each other in layers and functioning towards achieving certain goal right so this is like a schematic of what a deep neural network or a feet forward neural network would look like this is again not something new which has come up in the last eight to ten years although people have started discussing it a lot in the last eight to ten years look at it way back in one thousand, nine hundred and sixty-five sixty-eight propos something which looked very much like a modern deep neural network or a modern feed forward neural network and in many circles hes considered to be one of the founding fathers of modern deep learning right so uh thats about that sixty-eight right from one thousand, nine hundred and forty-three to one thousand, nine hundred and sixty-eight it was mainly about the springtime for ai and what i mean by that that everyone was showing interest in that the government was funding a lot of research in ai and people really thought that ai could deliver a lot of things on a lot of fronts for various applications healthcare defense and so on and then around one thousand, nine hundred and sixty-nine an interesting paper came out by these two gentlemen minsky and paper which essentially outlined some limitations of the perceptron model right and well talk about these limitations later on in the course in the second or third lecture but for now ill not get into the details of that but what it said that it is possible that a perceptron cannot handle some very simple functions also so youre trying to make the perceptron learn some very complex functions because the way we decide how to watch a movie is a very complex function of the inputs that we consider but even a simple function like xr is something which a perceptron cannot be used to model thats what this paper essentially showed and this led to severe criticism for ai and then people started losing interest in ai and a lot of government funding actually subsided after one thousand, nine hundred and sixty-nine all the way to one thousand, nine hundred and eighty-six actually this was the ai winter of connectionism so there was very little interest in connectionist ai so there are two types of ai one is symbolic ai and the other is connectionist ai so whatever we are going to study in this course about neural networks and all that probably falls in connectionist ai paradigm and there was no interest in this and people i mean its hard to get funding and so on for these seventeen to eighteen years and that was largely triggered by this study that was done by minsky and pepper and interestingly they were also often misquoted on what they had uh actually said in their paper so they had said a single perceptron cannot do it they in fact said that a multier network of perceptrons can do it but no one focused on the second part that a multier network of peron people started pushing the idea that a perceptron cannot do it and hence we should not be investigating it and so on right so thats what happened for a long time and this is known as the winter or the first winter then around one thousand, nine hundred and eighty-six actually came uh this algorithm which is known as back propagation again this is an algorithm which we are going to cover in a lot of detail in the course uh in the fourth or fifth lecture and this algorithm actually enables to train a deep neural network right so a deep network of neurons is something that you can train using this algorithm now this algorithm was actually popularized by r rumelhart and others in one thousand, nine hundred and eighty-six but it was not uh completely discovered by them this was also around in various other fields so it was there in uh i think in systems analysis or something like that it was being used for other purposes in a different context and so on and rumelhart other and others in one thousand, nine hundred and eighty-six were the first to kind of popularize it in the context of deep neural networks and this was a very important discovery because even today all the neural networks or most of them are trained using back propagation right and of course there have been several other advances but the core remains the same that you use back propagation to train a deep neural network right is something this was discovered almost thirty years back is still primarily used for training deep neural networks right thats why this was a very important uh paper or uh breakthrough at that time and around the same time so again interestingly so back propagation uh is used in conjunction with something known as gradient descent which was again discovered way back in one thousand, eight hundred and forty-seven by kosi and he was interested in using this to compute the orbit of heavenly bodies right that is something that people cared about at that time today of course we use it for various other purposes one of them being discovering cats in videos or even for medical imaging or for descripting uh whether a a certain type of cancer is being depicted in a xray or things like that theres a lot of other purposes for which deep neural networks enhance uh and hence back propagation gradient descent and other things are being used for right but again these are not very modern discoveries these are dated way back thirty years and even grent descent is almost one hundred and fifty years and so on right so thats uh what i wanted to emphasize and around the same time in one thousand, nine hundred and ninety or one thousand, nine hundred and eighty-nine theres this another interesting theorem which was proved which is known as the universal approximation theorem and this is again something that well cover in the course uh in the third lecture or something like that where well talk about the power of a deep neural network right so again the importance of this or why this theorem was important will become clear later on when we cover it in detail but for now its important to understand that what this theorem said is that if you have a deep neural network you could basically model all types of functions continuous functions to any desired precision so what it means in very lay and terms is that if the way you make decisions using a bunch of inputs is a very very complex function of the input then you can have a neural network which will be able to learn this function right in many layman terms that is what it means and if i have to hype it up a bit or have to say it in a very enthused and excited manner i would say that basically it says that ne deep neural networks can be used for solving all kinds of machine learning problems and thats roughly what it says but with a pinch of salt and lot of caveats but that is uh what it means at least in the context of this course right so this is all around one thousand, nine hundred and eighty-nine and despite this happening right some important discoveries towards the late end of eightys which was back propagation universal approximation theorem people were still not uh being able to use deep neural networks for really solving large uh practical problems right and a few challenges there was of course the compute power at that time was not at a level where uh it could support deep neural networks we did not have enough data for training deep neural networks and also in terms of techniques while back propagation is a sound technique it is to fail when you have really deep neural network so when people tried training a very deep neural network they found that the training does not really converge the system does not really learn anything and so on and there were certain issues with using back propagation off the shelf at that time because of which it was not very successful right so again despite these slight boom around eighty-six to ninety where some important discoveries were made and even followup in ninety-two ninety-three and so on theres still not a real big hype around deep neural networks or artificial neural networks and that there was again a slum a slow winter uh right up till two thousand and six music n music music"}
