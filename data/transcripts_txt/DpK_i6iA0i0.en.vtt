WEBVTT
Kind: captions
Language: en

00:00:13.000 --> 00:00:18.270
When this deep revival happened, right so
in 2006 a very important study was or a very

00:00:18.270 --> 00:00:22.839
important contribution was made by Hinton
and Salakhutdinov.

00:00:22.839 --> 00:00:27.160
.
Sorry, if I have not pronounced it properly

00:00:27.160 --> 00:00:31.810
and they found that a method for training
very deep neural network effectively.

00:00:31.810 --> 00:00:33.760
Now, again the details of these are not important.

00:00:33.760 --> 00:00:37.920
We will be doing that in the course at some
point, but what is the important take away

00:00:37.920 --> 00:00:40.890
here is that while from 1989 to 2006.

00:00:40.890 --> 00:00:44.680
We knew that there is an algorithm for training
deep neural networks and they can potentially

00:00:44.680 --> 00:00:49.910
be used for solving a wide range of problems
because that is what the universal approximation

00:00:49.910 --> 00:00:54.070
theorem said, but the problem was that in
practice we were not being able to use it

00:00:54.070 --> 00:00:55.070
for much, right.

00:00:55.070 --> 00:01:00.470
It was not easy to train these networks, but
now with this technique there was revived

00:01:00.470 --> 00:01:04.949
interest and hope that now actually can train
very deep neural networks for lot of practical

00:01:04.949 --> 00:01:07.950
problems, this sparked off the interest again.

00:01:07.950 --> 00:01:12.951
And then, people started looking at all such
of thing, right that even this particular

00:01:12.951 --> 00:01:17.180
study which was done in 2006 will actually
be very simple to something done way back

00:01:17.180 --> 00:01:23.659
in 91-93 and which again showed that you can
train a very deep neural network.

00:01:23.659 --> 00:01:27.350
But again due to several factors may be at
that time due to the computational requirements

00:01:27.350 --> 00:01:31.270
or the data requirements or whatever I am
not too sure about that, it did not become

00:01:31.270 --> 00:01:37.240
so popular then, but by 2006 probably the
stage was much better for these kind of networks

00:01:37.240 --> 00:01:38.549
or techniques to succeed.

00:01:38.549 --> 00:01:40.900
So, then it became popular in 2006.

00:01:40.900 --> 00:01:44.810
.
Then, this 2006 to 2009 people started gaining

00:01:44.810 --> 00:01:50.170
more and more insights into the effectiveness
of this discovery made by Hinton and others

00:01:50.170 --> 00:01:51.909
which is unsupervised pre-training, right.

00:01:51.909 --> 00:01:56.200
That is what I spoke about on the previous
slide unsupervised pre-training.

00:01:56.200 --> 00:02:01.149
And they started getting more and more insights
into how you can make deep neural networks

00:02:01.149 --> 00:02:02.149
really work, right.

00:02:02.149 --> 00:02:05.530
So, they came up with various techniques,
some of which we are going to study in this

00:02:05.530 --> 00:02:06.530
course.

00:02:06.530 --> 00:02:09.970
So, this was about how do you initialise the
network better, what is the better optimization

00:02:09.970 --> 00:02:14.310
algorithm to use, what is the better regularization
algorithm to use and so on.

00:02:14.310 --> 00:02:21.420
So, there were many things which were started
coming out at this period 2006 to 2009 and

00:02:21.420 --> 00:02:26.560
by 2009, everyone started taking note of this
and again deep neural networks of artificial

00:02:26.560 --> 00:02:28.200
neural networks started becoming popular.

00:02:28.200 --> 00:02:32.940
That is when people realised that all this,
all the negative things that were tied to

00:02:32.940 --> 00:02:35.530
it that you are not able to train it well
and so on.

00:02:35.530 --> 00:02:40.170
Have slowly people have started finding solutions
to get by those and maybe we should start

00:02:40.170 --> 00:02:44.620
again focusing on the potential of deep neural
networks and see if they can be used for large

00:02:44.620 --> 00:02:46.080
scale practical application, right.

00:02:46.080 --> 00:02:51.400
So, this 2006 to 2009 was again a slow boom
period were people were again trying to do

00:02:51.400 --> 00:02:56.390
a lot of work to popularize deep neural networks
and get rid of some of the problems which

00:02:56.390 --> 00:02:57.860
existed in training them.

00:02:57.860 --> 00:03:05.610
Now, from 2009 onwards there was this series
of success is which kind of caught everyone

00:03:05.610 --> 00:03:10.080
which made everyone to stand up and take notice,
right that this is really working for a lot

00:03:10.080 --> 00:03:12.810
of practical applications starting with handwriting
recognition.

00:03:12.810 --> 00:03:19.390
So, around 2009, these guys won handwriting
recognition competition in Arabic and they

00:03:19.390 --> 00:03:23.769
did way better than the competitor systems
using a deep neural network and then, this

00:03:23.769 --> 00:03:24.769
was a success.

00:03:24.769 --> 00:03:25.900
.
So, this was an handwriting recognition and

00:03:25.900 --> 00:03:28.379
then, there was speech.

00:03:28.379 --> 00:03:35.409
So, this shown that various existing systems,
the error rate of these system could be seriously

00:03:35.409 --> 00:03:41.291
be significantly reduced by using deep neural
networks or plugging in a deep neural network

00:03:41.291 --> 00:03:42.810
component to existing systems, right.

00:03:42.810 --> 00:03:46.059
So, this was handwriting and then, speech.

00:03:46.059 --> 00:03:47.890
.
Then again some kind of pattern recognition

00:03:47.890 --> 00:03:53.180
which was on handwritten digit recognition
for MNIST; this is a very popular data set

00:03:53.180 --> 00:03:57.970
which had been around since 98 and a new record
was set on this data.

00:03:57.970 --> 00:04:04.930
So, this is the highest accuracy that was
achieved on this data set around that time

00:04:04.930 --> 00:04:10.769
in 2010, sorry and this is also the time when
GPUs entered the same, right.

00:04:10.769 --> 00:04:15.900
So, before that all of the stuff was being
done on CPUs, but then people realised that

00:04:15.900 --> 00:04:20.080
very deep neural networks require a lot of
computation and lot of this computation can

00:04:20.080 --> 00:04:22.840
happen very quickly on GPUs as opposed to
CPUs.

00:04:22.840 --> 00:04:26.910
So, people started using GPUs for training
and that drastically reduced the training

00:04:26.910 --> 00:04:27.910
and inference time.

00:04:27.910 --> 00:04:32.060
So, that was again something which sparked
a lot of interest, right because even though

00:04:32.060 --> 00:04:35.150
these were successful, they were taking a
lot of time to train, but now the GPUs could

00:04:35.150 --> 00:04:38.500
even take care of that and this success continued.

00:04:38.500 --> 00:04:42.340
.
So, people started gaining or getting success

00:04:42.340 --> 00:04:44.350
in other fields like visual pattern recognition.

00:04:44.350 --> 00:04:49.091
So, this was a competition on recognising
traffic sign boards and here again a deep

00:04:49.091 --> 00:04:54.060
neural network did way better than its other
competitors.

00:04:54.060 --> 00:04:56.360
.
And then, the most popular or one thing which

00:04:56.360 --> 00:05:01.440
made neural networks really popular was this
Image Net Challenge which was around since

00:05:01.440 --> 00:05:09.080
2008 or 2009 and before 2012 when this AlexNet
was one of the participating systems in this

00:05:09.080 --> 00:05:16.110
competition, most of the systems were non
neural network based systems and this competition

00:05:16.110 --> 00:05:21.900
was basically about classifying a given image
into one of thousand classes, right.

00:05:21.900 --> 00:05:26.460
So, this could be an image of a bird or a
dog or a human or car, truck and so on say

00:05:26.460 --> 00:05:31.460
you have to identify the right class of the
main object in the image, right.

00:05:31.460 --> 00:05:37.880
So, in 2012 this AlexNet which was a deep
neural network or a convolutional neural network

00:05:37.880 --> 00:05:44.790
based system was able to actually outperform
all the other systems by a margin of 67 percent,

00:05:44.790 --> 00:05:45.790
right.

00:05:45.790 --> 00:05:49.680
So, the error for this system was 16 percent
and this is a deep neural network because

00:05:49.680 --> 00:05:51.930
it had 8 layers.

00:05:51.930 --> 00:05:55.690
The next year this was improved further and
something known as ZF network propose which

00:05:55.690 --> 00:05:59.030
was again 8 layers, but it did better than
AlexNet.

00:05:59.030 --> 00:06:04.190
The next year even a deeper network with 19
layers was proposed which did significantly

00:06:04.190 --> 00:06:05.630
better than AlexNet.

00:06:05.630 --> 00:06:10.569
Then, Google entered the scene and they proposed
something which is 22 layers and again reduced

00:06:10.569 --> 00:06:16.100
the error, then Microsoft joined in and they
proposed something which had 152 layers and

00:06:16.100 --> 00:06:19.889
the error that you see here is actually better
than what humans do, right.

00:06:19.889 --> 00:06:24.780
So, even if a human was asked to label this
image because of certain law, certain noise

00:06:24.780 --> 00:06:29.490
in the image and so on, even a human is bound
to make more errors than 3.6 percent, right.

00:06:29.490 --> 00:06:34.620
That means, even if you show hundred images
to humans, he or she is bound to may go wrong

00:06:34.620 --> 00:06:38.979
or more than three or four of these images
right there is this system was able to get

00:06:38.979 --> 00:06:43.570
an error of 3.6 percent over the large test
set.

00:06:43.570 --> 00:06:49.490
So, this 2012 to 2016 period were there was
this continuous success on the Image Net Challenge

00:06:49.490 --> 00:06:54.600
as well as successes in other fields like
Natural Language Processing, Handwriting recognition,

00:06:54.600 --> 00:06:55.600
Speech and so on.

00:06:55.600 --> 00:06:59.110
So, this is the period where now everyone
started talking about deep learning and lot

00:06:59.110 --> 00:07:01.400
of company started investing in it.

00:07:01.400 --> 00:07:06.419
A lot of traditional systems which were not
deep neural network based was now started,

00:07:06.419 --> 00:07:09.900
people started converting them to deep neural
network based system, right.

00:07:09.900 --> 00:07:14.160
So, translation system, speed systems, image
classification, object detection and so on,

00:07:14.160 --> 00:07:19.220
there were lot of success in all these fields
using deep neural networks, right.

00:07:19.220 --> 00:07:23.140
And this particular thing that we are talking
about which is image net and the success in

00:07:23.140 --> 00:07:26.449
this was driven by something known as Convolutional
Neural Networks.

