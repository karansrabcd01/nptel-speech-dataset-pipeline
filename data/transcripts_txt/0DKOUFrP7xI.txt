music so now so this is what the progression was rate that in two thousand and six people started or the study by hinton and others led to the survival and then people started realizing that deep neural networks and actually we use for a lot of practical applications and actually beat a lot of existing systems but theres still some problems and we still need to make the system more robust faster and even scale higher accuracies and so on right so impera nearly vile there was a lot of success happening from two thousand and twelve to two thousand and sixteen or even two thousand and ten to two thousand and sixteen in peril there was also a lot of research to find better optimization algorithms which could lead to better convergence better accuracies and again some of the older ideas which were proposed way back in one thousand, nine hundred and eighty-three now this is again something that we will do in the course so most of the things that i am talking about we are going to cover in the course so we are going to talk about the amazing it challenge theyre going to talk about all those networks the winning networks that i had listed there alex needs a definite google net and so on we are going to talk about nesterov gradient descent which is listed on this slide and many other better optimization methods which were proposed starting from two thousand and eleven so there was this parallel resource happening while people were getting a lot of success using traditional neural networks theres also interested in making them better and robust and lead for lead to faster convergence and better accuracies and so on so this led to a lot of interest in coming up with better optimization algorithms and there was a series of these proposed starting from two thousand and eleven so a degrade is again something that well do in the course rmsprop adam eve and many more right so many new algorithms have been proposed and in parallel a lot of other regularization techniques or weight initialization strategies have also been proposed for example batch normalization or xaviar initialization and so on so these are all things which were aimed at making neural networks perform even better or faster and even reach better solutions or better accuracies and so on so this is all that we are going to see in the course at some point of the other music you