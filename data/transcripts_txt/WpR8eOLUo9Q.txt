music and now what well get into uh in the next chapter is well start talking about artificial intelligence and this is titled as from the spring to the winter of ai so im going to talk about when was this boom in ai started or when is that people started thinking and talking about ai seriously and what event happened to the initial boom and so on right so lets start with one thousand, nine hundred and forty-three where as i was saying that there was a lot of interest in understanding how does the human brain work and then come up with a computational or a mathematical model of that right so molik and pits one of them was a neuroscientist and the other one was a logician right so no computer scientists or anything at that point of time and they came up with this extremely simplified model that just as a brain takes the input from lot of fact s right so now suppose you want to decide whether you want to go out for a movie or not so you would probably think about do you really have any exams coming up that could be your factor xone you could think about is the weather good to go out is it training it would be difficult to go out at this point would there be a lot of traffic is it a very popular movie and hence tickets may not be available and so on right so a brain kind of processes all this information you might also look at things like the reviews of the movie or the imdb rating of the movie and so on and based on all these complex inputs it applies some function and then takes a decision yes or no that okay i want to probably go for a movie so this is an overly simplified model of how the brain works is and what this model says is that you take inputs from various sources and based on that you come up with a binary decision right so this is what they proposed in one thousand, nine hundred and forty-three so now we have come to an artificial neurons so this is not a biological neuron this is how you would implement it as a machine right so that was in one hundred and ninety-four three then along and then this kind of led to a lot of uh boom uh in our interest in artificial intelligence and so on and i guess around one thousand, nine hundred and fifty-six uh in a conference the term artificial intelligence was forly coined and within a one or two years from there frank rosenal came up with this perceptron model of uh doing computations or perceptron model of what an artificial neuron could be and well talk about this in detail later on the course i tell you what these things are as of now just think of the a new model was proposed and this is what he had to say about this model right so he said that the perceptron may eventually be able to learn make decisions and translate languages you find anything odd about this statement yeah so learn and make decisions make sense but why translate languages why so specific why such a specific interest in languages right so that you have to connect back to history right so this is also the the period of the cold war and theres always a lot of interest a lot of research in translation was actually fueled by the world war and events that happened after that where these uh countries which were at loggerheads with each other they wanted to understand what the other countries doing but they did not speak each others language thats why there was a lot of interest from spon point of view or from spying and so on to be able to translate languages and hence that specific required and a lot of this research would have been funded from agencies which are interested in these things right in the defense or war or something okay so and this work was largely done for the navy and this is an uh this is an extract from the article written in new york times way back in one thousand, nine hundred and fifty-seven or fifty-eight where it was mentioned that the embryo of an this perceptron is an embryo of an electronic computer that the navy expects will be able to walk talk see write reproduce itself and be conscious of it existence so im not quoting something from two thousand and seventeen or eighteen this is way back in one thousand, nine hundred and fifty-seven fifty-eight right and thats why i like the history part of it so recently theres a lot of uh boom or a lot of hype around ai that ai will take over a lot of things itll take over jobs it might eventually uh we might be colonized by ai agents and so on so i just want to emphasize that i dont know whether that will happen or not but this is not something new we have been talking about the promise of ai as far back since one thousand, nine hundred and fifty-seven one thousand, nine hundred and fifty-eight right this is not something new that people are talking about now its always been there and to what extent this promise will be fulfilled is yet to be seen and of course as compared to one thousand, nine hundred and fifty-seven fifty-eight we have made a lot of progress in other fields which have enabled ai to be uh much more successful than it was earlier for example we have much better compute power now we have lots of data now thanks to the internet and other things that you can actually crawl tons and tons of data and then try to learn something from a data or try to make the machine learn something from data right so we have made a lot of progress in other aspects because of which ai is now at a position where it can really make a difference but just wanted to say that these are not things which have not been said in the past it has always been that a has always been considered to be very promising and perhaps a bit hyped also right so thats about one thousand, nine hundred and fifty-seven fifty-eight then now what we talk about or the for the past eight to ten years at least when we talk about ai talking about deep learning and that is what this course is about largely about deep learning im not saying that other and what deep learning uh is largely about if i want to tell you in a very layman nutshell term is its about a large number of artificial neurons connected to each other in layers and functioning towards achieving certain goal right so this is like a schematic of what a deep neural network or a feet forward neural network would look like this is again not something new which has come up in the last eight to ten years although people have started discussing it a lot in the last eight to ten years look at it way back in one thousand, nine hundred and sixty-five sixty-eight propos something which looked very much like a modern deep neural network or a modern feed forward neural network and in many circles hes considered to be one of the founding fathers of modern deep learning right so uh thats about that sixty-eight right from one thousand, nine hundred and forty-three to one thousand, nine hundred and sixty-eight it was mainly about the springtime for ai and what i mean by that that everyone was showing interest in that the government was funding a lot of research in ai and people really thought that ai could deliver a lot of things on a lot of fronts for various applications healthcare defense and so on and then around one thousand, nine hundred and sixty-nine an interesting paper came out by these two gentlemen minsky and paper which essentially outlined some limitations of the perceptron model right and well talk about these limitations later on in the course in the second or third lecture but for now ill not get into the details of that but what it said that it is possible that a perceptron cannot handle some very simple functions also so youre trying to make the perceptron learn some very complex functions because the way we decide how to watch a movie is a very complex function of the inputs that we consider but even a simple function like xr is something which a perceptron cannot be used to model thats what this paper essentially showed and this led to severe criticism for ai and then people started losing interest in ai and a lot of government funding actually subsided after one thousand, nine hundred and sixty-nine all the way to one thousand, nine hundred and eighty-six actually this was the ai winter of connectionism so there was very little interest in connectionist ai so there are two types of ai one is symbolic ai and the other is connectionist ai so whatever we are going to study in this course about neural networks and all that probably falls in connectionist ai paradigm and there was no interest in this and people i mean its hard to get funding and so on for these seventeen to eighteen years and that was largely triggered by this study that was done by minsky and pepper and interestingly they were also often misquoted on what they had uh actually said in their paper so they had said a single perceptron cannot do it they in fact said that a multier network of perceptrons can do it but no one focused on the second part that a multier network of peron people started pushing the idea that a perceptron cannot do it and hence we should not be investigating it and so on right so thats what happened for a long time and this is known as the winter or the first winter then around one thousand, nine hundred and eighty-six actually came uh this algorithm which is known as back propagation again this is an algorithm which we are going to cover in a lot of detail in the course uh in the fourth or fifth lecture and this algorithm actually enables to train a deep neural network right so a deep network of neurons is something that you can train using this algorithm now this algorithm was actually popularized by r rumelhart and others in one thousand, nine hundred and eighty-six but it was not uh completely discovered by them this was also around in various other fields so it was there in uh i think in systems analysis or something like that it was being used for other purposes in a different context and so on and rumelhart other and others in one thousand, nine hundred and eighty-six were the first to kind of popularize it in the context of deep neural networks and this was a very important discovery because even today all the neural networks or most of them are trained using back propagation right and of course there have been several other advances but the core remains the same that you use back propagation to train a deep neural network right is something this was discovered almost thirty years back is still primarily used for training deep neural networks right thats why this was a very important uh paper or uh breakthrough at that time and around the same time so again interestingly so back propagation uh is used in conjunction with something known as gradient descent which was again discovered way back in one thousand, eight hundred and forty-seven by kosi and he was interested in using this to compute the orbit of heavenly bodies right that is something that people cared about at that time today of course we use it for various other purposes one of them being discovering cats in videos or even for medical imaging or for descripting uh whether a a certain type of cancer is being depicted in a xray or things like that theres a lot of other purposes for which deep neural networks enhance uh and hence back propagation gradient descent and other things are being used for right but again these are not very modern discoveries these are dated way back thirty years and even grent descent is almost one hundred and fifty years and so on right so thats uh what i wanted to emphasize and around the same time in one thousand, nine hundred and ninety or one thousand, nine hundred and eighty-nine theres this another interesting theorem which was proved which is known as the universal approximation theorem and this is again something that well cover in the course uh in the third lecture or something like that where well talk about the power of a deep neural network right so again the importance of this or why this theorem was important will become clear later on when we cover it in detail but for now its important to understand that what this theorem said is that if you have a deep neural network you could basically model all types of functions continuous functions to any desired precision so what it means in very lay and terms is that if the way you make decisions using a bunch of inputs is a very very complex function of the input then you can have a neural network which will be able to learn this function right in many layman terms that is what it means and if i have to hype it up a bit or have to say it in a very enthused and excited manner i would say that basically it says that ne deep neural networks can be used for solving all kinds of machine learning problems and thats roughly what it says but with a pinch of salt and lot of caveats but that is uh what it means at least in the context of this course right so this is all around one thousand, nine hundred and eighty-nine and despite this happening right some important discoveries towards the late end of eightys which was back propagation universal approximation theorem people were still not uh being able to use deep neural networks for really solving large uh practical problems right and a few challenges there was of course the compute power at that time was not at a level where uh it could support deep neural networks we did not have enough data for training deep neural networks and also in terms of techniques while back propagation is a sound technique it is to fail when you have really deep neural network so when people tried training a very deep neural network they found that the training does not really converge the system does not really learn anything and so on and there were certain issues with using back propagation off the shelf at that time because of which it was not very successful right so again despite these slight boom around eighty-six to ninety where some important discoveries were made and even followup in ninety-two ninety-three and so on theres still not a real big hype around deep neural networks or artificial neural networks and that there was again a slum a slow winter uh right up till two thousand and six music n music music